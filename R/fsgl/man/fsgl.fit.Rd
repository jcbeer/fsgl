% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fsgl.fit.R
\name{fsgl.fit}
\alias{fsgl.fit}
\title{Fused Sparse Group Lasso}
\usage{
fsgl.fit(X, Y, K, nj, nd, ngroups, groupsizes, alpha, gamma, lambda,
  beta0 = NULL, theta0 = NULL, mu0 = NULL, beta_update_factor = NULL,
  beta_update_term1 = NULL, rho = 1, Niter = 2000, epsilon_abs = 10^-3,
  epsilon_rel = 10^-3, verbose = FALSE)
}
\arguments{
\item{X}{a n*p matrix of predictor variables with observations in rows.}

\item{Y}{a n*1 vector of the response variable.}

\item{K}{a (nj + nd + ng)*p matrix encoding the lasso penalty (first nj rows), the graph structure for the fused penalty (the next nd rows), and the group structure for the group penalty (last ng rows). Can be made with function \code{makeKmatrix}.}

\item{nj}{number of rows of K that encode the lasso penalty. If lasso penalty is applied to all coefficients then this will equal p.}

\item{nd}{number of rows of K that encode the graph structure for the fused penalty.}

\item{ngroups}{number of groups for the group penalty.}

\item{groupsizes}{a vector of length ngroups that gives the size of each group in the order they appear in the K matrix. Sum should equal ng.}

\item{alpha}{tuning parameter that controls the degree of group (alpha = 0) vs L1 (alpha=1) sparsity. 0 <= alpha <= 1}

\item{gamma}{tuning parameter that controls the degree of sparsity (gamma=1) vs fusion (gamma=0) penalty. 0 <= gamma <= 1}

\item{lambda}{tuning parameter that controls the overall degree of regularization.}

\item{beta0}{starting values for beta. Defaults to zero vector.}

\item{theta0}{starting values for theta. Defaults to zero vector.}

\item{mu0}{starting values for mu. Defaults to zero vector.}

\item{beta_update_factor}{by default function will set this equal to \code{solve(t(Xcentered) \%*\% Xcentered + rho * t(K) \%*\% K)}. It may take a long time to compute and does not depend on tuning parameters, so it may be provided as an argument if fitting the model at many sets of tuning parameters.}

\item{rho}{step size for ADMM algorithm. Defaults to 1.}

\item{Niter}{number of ADMM iterations. Defaults to 2000.}

\item{epsilon_abs}{absolute tolerance for ADMM convergence. Defaults to 10^-3.}

\item{epsilon_rel}{relative tolerance for ADMM convergence. Defaults to 10^-3.}

\item{verbose}{if TRUE will print number of ADMM iterations used. Defaults to FALSE.}

\item{beta_update_term}{by default function will set this equal toset equal to \code{t(Xcentered) \%*\% Ycentered}. Does not depend on tuning parameters, so it may be provided as an argument if fitting the model at many sets of tuning parameters.}
}
\description{
Fits a regression model for one set of tuning parameter values by minimizing a penalized least squares loss function.
}
\examples{
Kmatrix <- makeKmatrix2d(dim1=3, dim2=3, groups=c(1, 2, 3, 2, 2, 1, 3, 3, 1))
x <- matrix(rnorm(54), nrow=6)
y <- x \%*\% c(0, 0, 10, 0, 0, 0, 10, 10, 0)
fsgl.fit(X=x, Y=y, K=Kmatrix$K, nj=Kmatrix$nj, nd=Kmatrix$nd, ngroups=Kmatrix$ngroups, groupsizes=Kmatrix$groupsizes, alpha=0.1, gamma=0.8, lambda=5)
}
\keyword{lasso}
